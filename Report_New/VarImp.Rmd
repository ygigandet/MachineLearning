# Variable of Importance

```{r, echo = FALSE, message = FALSE}
source(here::here("scripts/setup.R"))
```

```{r, echo = FALSE, message = FALSE, warning = FALSE}
#  Load the cleaned dataset from the data.Rmd
setwd(here::here("Data_New/"))
premier_league_results <- read_csv("premier_league_results_clean.csv")
```

Variable importance is a method that provides a measure of the importance of each feature for the model prediction quality. It helps to analyze the link between the features and the response, through the model predictions. 
Our variable importance is model-specific since the approaches that are specific to the model that is used. We have apply variable of importance on our 3 best models: NN, CART, NB. To do so, we create an explain object with DALEX :: explain function, then implement a variable importance calculation. 

Studies show that a higher score means that the specific feature will have a larger effect on the model that is being used to predict a certain variable. Based on this principle, we can see that either in CART or NN, half time results is an important variable. Indeed, a change on this variable can drastically change the result of both models. Then, we have half time away goal followed by away shoot on target.
For each feature, the difference in entropy obtained by 10 permutations is shown by the following plot. It shows graphically the average importance variable according to the cross entropy loss.

```{r, echo = FALSE, message = FALSE, warning = FALSE}
# Take the NN machine model 
nnetFit

# Take the CART machine model 
classtreefit.pruned

# Take the NB machine model 
naivebayesfit

#Creating an explain object
x_train <- select(train_data1, -full_time_results)
y_train <- pull(train_data1, full_time_results)

explainer_nn <- DALEX::explain(model = nnetFit,
                                is_multiclass = TRUE,
                                data = x_train,
                                y = y_train,
                                label = "NN machine model")
  #NN fonctionne

explainer_cart <- DALEX::explain(model = classtreefit.pruned,
                                is_multiclass = TRUE,
                                data = x_train,
                                y = y_train,
                                label = "CART machine model ")
  #CART fonctionne

explainer_nb <- DALEX::explain(model = naivebayesfit,
                                is_multiclass = TRUE,
                                data = x_train,
                                y = as.numeric(y_train),
                                label = "NB machine model")

  #Problem : erreur Model info detected regression task but 'y' is a factor . (  WARNING  ), so i turn into as.numeric
  # Another error : the predict_function returns an error when executed (  WARNING  ) 
```


# Plotting the feature importance
```{r, echo = FALSE, message = FALSE, warning = FALSE}
calculate_importance <- function(your_model_explainer, n_permutations = 10) {
  imp <- model_parts(explainer = your_model_explainer,
                     B = n_permutations,
                     type = "ratio",
                     N = NULL)
  return(imp)
}

importance_nn <- calculate_importance(explainer_nn)
importance_cart <- calculate_importance(explainer_cart)
importance_nb <- calculate_importance(explainer_nb)

plot(importance_nn, importance_cart) +
 ggtitle("Mean variable-importance ratio over 10 permutations", "")

```

We know that smaller the entropy is, better is the model. In term of cross entropy, ....


Once weâ€™ve identified influential variables across all three models, next we likely want to understand how the relationship between these influential variables and the average prediction response differ between the models.

To do so, we can run a partial dependence profile

```{r}
#
adp_nn <- model_profile(explainer_nn, 
 variables = "half_time_results")
plot(adp_nn)

adp_cart <- model_profile(explainer_cart, 
 variables = "half_time_results")
plot(adp_cart)

#PLot 
plot(adp_nn$agr_profiles, 
 adp_cart$agr_profiles)

plot(adp_nn, adp_cart) +
 ggtitle("Partial dependence profile", "")

```
Except one outlier in red (NN predicting H) which is high in every case, and will predict mostly Home even if it's A, D or H. The behavior of those models is very similar, we can observe a low partial dependance of Away and Home, indeed when one is up, the other one is down. Therefore, we can say that both models have the same interactions in the data, although using the cart model is a little bit better.


```{r}
#Recursive feature elimination
rfeCtrl <- rfeControl(functions = rfFuncs,
                      method = "cv",
                      verbose = FALSE)

rfProfile <- rfe(x = train_data1, 
                y = as.vector(as.numeric(train_data1$full_time_results)),
                rfeControl = rfeCtrl)
rfProfile
```
