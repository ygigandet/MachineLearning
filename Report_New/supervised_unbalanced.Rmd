# Supervised Learning

```{r, echo = FALSE, message = FALSE}
source(here::here("scripts/setup.R"))
```

```{r, echo = FALSE, message = FALSE, warning = FALSE}
#  Load the cleaned dataset from the data.Rmd
setwd(here::here("Data_New/"))
premier_league_results <- read_csv("premier_league_results_clean.csv")
```

Next step is fitting models to the clean data, based on our EDA outcomes. But before, to avoid overfitting, we need to split our data into a training and a test sets.

## Data Splitting

For our splitting strategy, we took 80% of the original data for our training set and the remaining 20% for our test set. As we have a large amount of data, we then went on further with a cross validation (CV) strategy on our training set before fitting our models.

Moreover, it is not necessary to balance the training set, because we are in a non-binary case, and the interpretation of the results will be more complex and the accuracies of the models are decreasing. For these reasons, we decided to keep original values without balancing.

```{r}
# Just removing the match data, full_time_home_goals and full_time_away_goals

premier_league_results <- premier_league_results %>% 
  select(-match_date, -full_time_home_goals, -full_time_away_goals)

# Need to factorize some variables - namely the characters
premier_league_results$full_time_results = as.factor(premier_league_results$full_time_results)
premier_league_results$half_time_results = as.factor(premier_league_results$half_time_results)
premier_league_results$home_team = as.factor(premier_league_results$home_team)
premier_league_results$away_team = as.factor(premier_league_results$away_team)

# Set the seeds to have the same samples over and over

set.seed(234)

train_indexes1 = createDataPartition(premier_league_results$full_time_results,p=0.8,list=FALSE)

train_data1 = premier_league_results[train_indexes1,]

test_data1 = premier_league_results[-train_indexes1,]
```

## Model fitting

Since we are trying to predict whether the result of a football match is either a win for the home team, a win for the away team or a draw, our models should be adapted to the case of a multiclass classification task. In this next part, we will fit multiple models and choose at the end the best 2 models, according to following metrics: accuracy and Cohen's Kappa. Therefore we are looking for models with high accuracy, meaning that the predicted variables match the observed variables as much as possible, and a high Cohen's Kappa, which is an indicator whether the model in hand is a better or different model than a random one.

### Naive Bayes

To fit our first model, namely Naive Bayes, we use the following code from the package `caret`:

```{r, echo = TRUE, warning = FALSE, results = 'hide'}
set.seed(1)

trctrl <- trainControl(method = "cv", number=10) # Cross-validation
search_grid <- expand.grid( # Hypertuning parameters
  usekernel = c(TRUE, FALSE),
  laplace = c(1:5),
  adjust = seq(0, 5, by = 1)
)

naivebayesfit <- train(full_time_results ~., # Modelling
                 data = train_data1,
                 method = "naive_bayes",
                 trControl = trctrl,
                 tuneGrid = search_grid)
```

```{r, results = 'hide'}
naivebayespred <- predict(naivebayesfit, newdata = test_data1)

cm <- confusionMatrix(data=as.factor(naivebayespred), reference = test_data1$full_time_results)

# Store the metrics
accuracy_nb <- cm$overall['Accuracy']
kappa_nb <- cm$overall['Kappa']
```

We reach an accuracy of `r cm$overall['Accuracy']` and a Kappa of `r cm$overall['Kappa']`. 

### K-NN

We proceed the same way for all the following models, meaning that we are always hypertuning the parameters to ensure that our models are the best possible. Here is the code for the K-Nearest Neighbors:

```{r, echo = TRUE, results = 'hide'}
set.seed(1)

ctrl <- trainControl(method = "cv", number=10) # Cross-validation
search_grid <-  expand.grid(k = seq(from = 1, to = 150, by = 1)) # Hypertuning k

knn <- train(full_time_results ~., # Modelling
                 data = train_data1,
                 method = "knn",
                 trControl = ctrl, 
                 tuneGrid = search_grid)
```

```{r, results = 'hide'}
knnfit <- knn3(full_time_results ~ ., data=train_data1, k=knn$bestTune$k)

knnpred <- predict(knnfit, newdata = test_data1, type="class")

cm <- confusionMatrix(data=as.factor(knnpred), reference = test_data1$full_time_results)
cm

# Store the metrics
accuracy_knn <- cm$overall['Accuracy']
kappa_knn <- cm$overall['Kappa']
```

After hypertuning the k parameter, AKA the number of closest neighbors, we fitted model with the optimal parameter k=`r knn$bestTune$k`, giving a convenient prediction. We have an accuracy of `r cm$overall['Accuracy']` and a low Kappa: `r cm$overall['Kappa']`. This result is worse than the Naive Bayes model.

### Classification Tree

For the classification tree, we need to hypertune the cp as in the code below:
```{r, echo = TRUE, results = 'hide'}
saveRDS(train_data1, "train_data.rds")
saveRDS(test_data1, "test_data.rds")
```

```{r, echo = TRUE, results = 'hide'}
set.seed(1)

ctrl <- trainControl(method = "cv", number=10) # Cross validation
search_grid <-  expand.grid(cp = seq(from = 0.005, to = 0.35, by = 0.005))
# Hypertuning the parameter 

tree <- train(full_time_results ~., # Modelling
              data = train_data1,
              method = "rpart",
              trControl = ctrl,
              tuneGrid = search_grid)
```

```{r, results = 'hide'}
classtreepred <- predict(tree, newdata=test_data1)

cm <- confusionMatrix(data=as.factor(classtreepred), reference = test_data1$full_time_results)
cm

# Store the metrics
accuracy_tree <- cm$overall['Accuracy']
kappa_tree <- cm$overall['Kappa']
```

After finding the optimal cp=`r tree$bestTune$cp` with hypertuning for pruning, the CART model gives a convenient prediction. We got an accuracy of `r cm$overall['Accuracy']` and a moderate Kappa: `r cm$overall['Kappa']`. The performance of this model is actually better to the Naive Bayes!

### Neural Networks

```{r, echo = TRUE, results = 'hide'}
set.seed(1)

fitControl <- trainControl(method = "cv", number = 10) # cross-validation 
nnetGrid <-  expand.grid(size = seq(from = 1, to = 10, by = 1), # Hyperparameters
                        decay = seq(from = 0.1, to = 0.5, by = 0.1))

nnetFit <- train(full_time_results ~ .,  # Modelling
                 data = train_data1,
                 method = "nnet",
                 metric = "Accuracy",
                 tuneGrid = nnetGrid,
                 trControl = fitControl)
```

```{r, results = 'hide'}
p1 <-  plot(nnetFit)

pred_neural_network <-  predict(nnetFit, newdata = test_data1)
cm <- confusionMatrix(data=pred_neural_network, reference = test_data1$full_time_results)

# Store the metrics
accuracy_nnet <- cm$overall['Accuracy']
kappa_nnet <- cm$overall['Kappa']
```

After tuning the hyperparameters for our neural network model (size of `r nnetFit$bestTune$size` and weight decay of `r nnetFit$bestTune$decay`), we get a pretty good model for predicting our outcomes. An accuracy of `r cm$overall['Accuracy']` and a moderate Cohen's Kappa of `r cm$overall['Kappa']`. Compared to the previous models, neural network is the best model for now.

### Support Vector Machines

```{r, echo = TRUE, results = 'hide'}
set.seed(1)

trctrl <- trainControl(method = "cv", number=5, classProbs = T) # Cross-validation
search_grid <- expand.grid(C = c(0.01, 0.1, 1, 10, 100, 1000)) # Hyperparameters
 
svm_tuned <- train(full_time_results ~.,# Modelling
                   data = train_data1, 
                   method = "svmLinear",
                   trControl = trctrl,
                   tuneGrid = search_grid)
```

```{r, results = 'hide'}
svm.tuned.pred <- predict(svm_tuned, newdata = test_data1)
cm <- confusionMatrix(data=svm.tuned.pred, reference = test_data1$full_time_results)

# Store the metrics
accuracy_svm <- cm$overall['Accuracy']
kappa_svm <- cm$overall['Kappa']
```

We find that the best tune for C is equal to `r svm_tuned$bestTune$C`, and we obtain an accuracy of `r cm$overall['Accuracy']` and a pretty good Kappa: `r cm$overall['Kappa']`.

### Random Forests

Our last model is Random forests:

```{r, echo = TRUE, results = 'hide'}
set.seed(1)

trctrl <- trainControl(method = "cv", number=5) # Cross-validation
search_grid <- expand.grid(.mtry = c(1:15)) #Hyperparameters

rf <- train(full_time_results~., # Modelling
            data = train_data1,
            method = "rf",
            trControl = trctrl,
            tuneGrid = search_grid
)
```

```{r}
pred_rf <-  predict(rf, newdata = test_data1)
cm <- confusionMatrix(data=pred_rf, reference = test_data1$full_time_results)

# Store the metrics
accuracy_rf <- cm$overall['Accuracy']
kappa_rf <- cm$overall['Kappa']
```

For our random forest model, the hyperparameter is `r rf$bestTune$mtry` for our randomly selected predictors (aka mtry). After hypertuning our model, we got an accuracy of `r cm$overall['Accuracy']` and a Cohen's Kappa of `r cm$overall['Kappa']`.

### Summary of the models

We tried to fit several models to have the best one to predict our desired outcomes. Before moving to the next step, we need to compare each model with their respective metrics to choose the best one:

```{r}
# Create a table to show the variables
metrics = c(accuracy_nb, kappa_nb, accuracy_knn, kappa_knn, accuracy_tree, kappa_tree, accuracy_nnet, kappa_nnet, accuracy_svm, kappa_svm, accuracy_rf, kappa_rf)
cnames = c("Accuracy", "Kappa")
rnames = c("Naive Bayes", "K-nearest neighbors", "Tree", "Neural Network", "Support Vector Machines", "Random Forest")
model_output <- matrix(metrics, ncol = 2, byrow=TRUE, dimnames=list(rnames,cnames))
model_output <- model_output[order(model_output[,1],decreasing=TRUE),]

kable(model_output, digits = 2)  %>% 
  kable_styling(bootstrap_options = c("striped","condensed"), fixed_thead = T)
```

By having a look at the results, we see that Random Forest outperforms the other models by far, followed by Support Vector Machines and Neural Network. Therefore, we will analyse the variable importance for the two best models.


## Variable Importance

Now that we found that our best models are Random Forest models and Support Vector Machines, we need to see if we can improve their prediction quality, that we judge by accuracy and kappa, by getting rid of the variables that do not contribute to the prediction. To do this, we use model-specific variable importance methods.

```{r, echo = FALSE, message = FALSE, warning = FALSE, results = 'hide'}
#Creating an explain object
x_train <- select(train_data1, -full_time_results)
y_train <- pull(train_data1, full_time_results)

p_fun <- function(object, newdata){predict(object, newdata=newdata, type="prob")[,2]}

explainer_classif_rf <- DALEX::explain(rf, label = "rf",
                                       data = train_data1, 
                                       y = as.vector(as.numeric(train_data1$full_time_results)),
                                       predict_function = p_fun, 
                                       verbose = FALSE)

explainer_svm <- DALEX::explain(model = svm_tuned,
                                data = train_data1,
                                y = train_data1$full_time_results,
                                label = "SVM model")

calculate_importance <- function(your_model_explainer, n_permutations = 10) {
  imp <- model_parts(explainer = your_model_explainer,
                     B = n_permutations,
                     type = "ratio",
                     N = NULL)
  return(imp)
}

importance_rf <- calculate_importance(explainer_classif_rf)
importance_svm <- calculate_importance(explainer_svm)
```

*Feature importance for Random forest *

A feature is “important” if shuffling its values increases the model error, because in this case the model relied on the feature for the prediction. In our case we measure the error increase by 1-AUC (1 minus the area under the ROC curve). Features associated with a model error increase by a factor of 1. Indeed, it means that there is no change when we’re at 1 and variables were not important for predicting full time result.
In our case, variables which are not important are away corners and home corners. In opposite, the most important feature is half time results permuting full time result in an increase in 1-AUC by a factor of approximately 1.05.

```{r, echo = FALSE, message = FALSE, warning = FALSE}
plot(importance_rf)
```
By running a recursive feature elimination on the RF model, we can see that the top 5 variables (out of 18) are full_time_results, half_time_results, away_shots_ontarget, half_time_home_goals and half_time_away_goals . We are going to use these features in order to refit the model to see if we can improve the accuracy and Kappa.

```{r, echo=FALSE, warning=FALSE, results = 'hide'}
#Recursive feature elimination
rfeCtrl <- rfeControl(functions = rfFuncs,
                      method = "cv",
                      verbose = FALSE)

rfProfile <- rfe(x = train_data1, 
                y = as.vector(as.numeric(train_data1$full_time_results)),
                rfeControl = rfeCtrl)

rfProfile
```


```{r, echo = FALSE, message = FALSE, warning = FALSE}
plot(importance_svm)

# In the plot above, we have the difference in entropy obtained by 10 permutations for the SVM Model. It shows graphically the average variable importance according to the cross entropy loss. Here also, it is the same 4 important variables which are the most weight. Not including theses variables can cause a decrease of the cross-entropy for SVM models

```


*Fitting the RF model with its tops feature importance*

Once we’ve identified the most critical variables for prediction, we would like to verify that excluding all other variables increases the accuracy and kappa of our best 2 models, by refitting them.

```{r, echo=FALSE, message = FALSE, warning = FALSE}
# Random Forest (RF)
set.seed(1)
trctrl <- trainControl(method = "cv", number=5)
search_grid <- expand.grid(.mtry=rf$bestTune$mtry)
                           

VarImpRFfit <- train(full_time_results~ half_time_results + half_time_away_goals + half_time_home_goals + away_shots_ontarget, 
            data=train_data1,
            method="rf",
            trControl=trctrl,
            tuneGrid=search_grid
)

VarImpRFpred <-  predict(VarImpRFfit, newdata = test_data1)

refitconfmatrixRF <- confusionMatrix(data=VarImpRFpred, reference = test_data1$full_time_results)
oldconfmatrixRF <- confusionMatrix(data=pred_rf, reference = test_data1$full_time_results)
```

```{r, echo=FALSE,message = FALSE, warning = FALSE, out.width="100%"}
oldconfmatrixRF$overall %>%
  kbl(caption = "Metrics of the original model") %>%
  kable_classic_2(full_width = F)
refitconfmatrixRF$overall %>%
  kbl(caption = "Metrics of the refit models")%>%
  kable_classic_2(full_width = F)
```
When we refit the Random Forest model with the 4 most "important" variables, accuracy and kappa decrease, from 0.713 to 0.639 and from 0.529 to 0.424 respectively. This is change can not be ignored and might be causes by variable dependency, hence it is probably better to keep the original model in this case and observe the source of the unexpected outcomes.


*Variable dependency*

The correlation between 5 variables might be a reason to our unexpected results. For instance we can see a high correlation between half time result and half time home goal. Such correlation can create inter-dependency and the way we train our models has to integreate the different correlation.  

```{r, echo=FALSE,message = FALSE, warning = FALSE}
train_data1$half_time_results <- as.numeric(train_data1$half_time_results)
train_data1$full_time_results <-as.numeric(train_data1$full_time_results)

M <- train_data1 %>% dplyr::select(half_time_results, half_time_away_goals, half_time_home_goals, away_shots_ontarget, full_time_results)

MM <- cor(M %>% dplyr::select(where(is.numeric)))

corrplot(MM, method = 'number', tl.cex = 0.8)

# Using randomForest's partialPlot function
#partialPlot(VarImpRFfit, pred.data = VarImpRFpred, x.var = full_time_results)

```


*Fitting the SVM model with its tops feature importance*
```{r, echo=FALSE,message = FALSE, warning = FALSE, out.width="50%", results = 'hide'}
# Support Vector Machine (SVM)
set.seed(1)

trctrl <- trainControl(method = "cv", number=5, classProbs = T) # Cross-validation
VarImpGrid <- expand.grid(C = svm_tuned$bestTune$C) # Hyperparameters
 
VarImpSVMfit <- train(full_time_results ~half_time_away_goals + half_time_home_goals + away_shots_ontarget + home_shots_ontarget, # Modelling
                   data = train_data1, 
                   method = "svmLinear",
                   trControl=trctrl,
                   tuneGrid = VarImpGrid)

VarImpSVM.tuned.pred <- predict(VarImpSVMfit, newdata = test_data1)

```

```{r, echo=FALSE}
oldconfmatrixSVM <-confusionMatrix(data=svm.tuned.pred, reference = test_data1$full_time_results)
refitconfmatrixSVM <- confusionMatrix(data=VarImpSVM.tuned.pred, reference = test_data1$full_time_results)

oldconfmatrixSVM$overall
refitconfmatrixSVM$overall
```

As for the Support Vector Machine model with the 4 most "important" variables, accuracy and kappa decrease again, from 0.704 to 0.648 and from 0.512 to 0.381 respectively. This is change is very marginal though, so it still encourages us to take fewer variables for our modeling as simpler is often better !


