<style>
body {
text-align: justify}
</style>

<br>

# Supervised Learning

```{r, echo = FALSE, message = FALSE}
source(here::here("scripts/setup.R"))
```

```{r, echo = FALSE, message = FALSE, warning = FALSE}
#  Load the cleaned dataset from the data.Rmd
setwd(here::here("Data_New/"))
premier_league_results <- read_csv("premier_league_results_clean.csv")
```

Next step is fitting models to the clean data, based on our EDA outcomes. But before, to avoid overfitting, we need to split our data into a training and a test sets.

<br>

## Data Splitting

Before splitting our data, we need to take out the full time goals variables for the home and away teams as it doesn’t make sense to include them in our models. Indeed, they will heavily influence the predictions. We also take out the match date, as it doesn’t bring anything to our model.

For our splitting strategy, we took 80% of the original data for our training set and the remaining 20% for our test set. As we have a large amount of data, we then went on further with a cross validation (CV) strategy on our training set before fitting our models.

Moreover, it is not necessary to balance the training set, because we are in a non-binary case, and the interpretation of the results will be more complex and the accuracies of the models are decreasing. For these reasons, we decided to keep original values without balancing.

```{r}
# Just removing the match data, full_time_home_goals and full_time_away_goals

premier_league_results <- premier_league_results %>% 
  select(-match_date, -full_time_home_goals, -full_time_away_goals)

# Need to factorize some variables - namely the characters
premier_league_results$full_time_results = as.factor(premier_league_results$full_time_results)
premier_league_results$half_time_results = as.factor(premier_league_results$half_time_results)
premier_league_results$home_team = as.factor(premier_league_results$home_team)
premier_league_results$away_team = as.factor(premier_league_results$away_team)

# Set the seeds to have the same samples over and over

set.seed(234)

train_indexes1 = createDataPartition(premier_league_results$full_time_results,p=0.8,list=FALSE)

train_data1 = premier_league_results[train_indexes1,]

test_data1 = premier_league_results[-train_indexes1,]
```

<br>

## Model fitting

Since we are trying to predict whether the result of a football match is either a win for the home team, a win for the away team or a draw, our models should be adapted to the case of a multiclass classification task. In this next part, we will fit multiple models and choose at the end the best 2 models, according to following metrics: accuracy and Cohen's Kappa. Therefore we are looking for models with high accuracy, meaning that the predicted variables match the observed variables as much as possible, and a high Cohen's Kappa, which is an indicator whether the model in hand is a better or different model than a random one.

<br>

### Naive Bayes

To fit our first model, namely Naive Bayes, we use the following code from the package `caret`:

```{r, echo = TRUE, warning = FALSE, results = 'hide'}
set.seed(1)

trctrl <- trainControl(method = "cv", number=10) # Cross-validation
search_grid <- expand.grid( # Hypertuning parameters
  usekernel = c(TRUE, FALSE),
  laplace = c(1:5),
  adjust = seq(0, 5, by = 1)
)

naivebayesfit <- train(full_time_results ~., # Modelling
                 data = train_data1,
                 method = "naive_bayes",
                 trControl = trctrl,
                 tuneGrid = search_grid)
```

```{r, results = 'hide'}
naivebayespred <- predict(naivebayesfit, newdata = test_data1)

cm <- confusionMatrix(data=as.factor(naivebayespred), reference = test_data1$full_time_results)

# Store the metrics
accuracy_nb <- cm$overall['Accuracy']
kappa_nb <- cm$overall['Kappa']
```

We reach an accuracy of `r cm$overall['Accuracy']` and a Kappa of `r cm$overall['Kappa']`. 

<br>

### K-NN

We proceed the same way for all the following models, meaning that we are always hypertuning the parameters to ensure that our models are the best possible. Here is the code for the K-Nearest Neighbors:

```{r, echo = TRUE, results = 'hide'}
set.seed(1)

ctrl <- trainControl(method = "cv", number=10) # Cross-validation
search_grid <-  expand.grid(k = seq(from = 1, to = 150, by = 1)) # Hypertuning k

knn <- train(full_time_results ~., # Modelling
                 data = train_data1,
                 method = "knn",
                 trControl = ctrl, 
                 tuneGrid = search_grid)
```

```{r, results = 'hide'}
knnfit <- knn3(full_time_results ~ ., data=train_data1, k=knn$bestTune$k)

knnpred <- predict(knnfit, newdata = test_data1, type="class")

cm <- confusionMatrix(data=as.factor(knnpred), reference = test_data1$full_time_results)
cm

# Store the metrics
accuracy_knn <- cm$overall['Accuracy']
kappa_knn <- cm$overall['Kappa']
```

After hypertuning the k parameter, AKA the number of closest neighbors, we fitted model with the optimal parameter k=`r knn$bestTune$k`, giving a convenient prediction. We have an accuracy of `r cm$overall['Accuracy']` and a low Kappa: `r cm$overall['Kappa']`. This result is worse than the Naive Bayes model.

<br>

### Classification Tree

For the classification tree, we need to hypertune the cp as in the code below:

```{r, echo = TRUE, results = 'hide'}
set.seed(1)

ctrl <- trainControl(method = "cv", number=10) # Cross validation
search_grid <-  expand.grid(cp = seq(from = 0.005, to = 0.35, by = 0.005))
# Hypertuning the parameter 

tree <- train(full_time_results ~., # Modelling
              data = train_data1,
              method = "rpart",
              trControl = ctrl,
              tuneGrid = search_grid)
```

```{r, results = 'hide'}
classtreepred <- predict(tree, newdata=test_data1)

cm <- confusionMatrix(data=as.factor(classtreepred), reference = test_data1$full_time_results)
cm

# Store the metrics
accuracy_tree <- cm$overall['Accuracy']
kappa_tree <- cm$overall['Kappa']
```

After finding the optimal cp=`r tree$bestTune$cp` with hypertuning for pruning, the CART model gives a convenient prediction. We got an accuracy of `r cm$overall['Accuracy']` and a moderate Kappa: `r cm$overall['Kappa']`. The performance of this model is actually better to the Naive Bayes!

<br>

### Neural Networks

```{r, echo = TRUE, results = 'hide'}
set.seed(1)

fitControl <- trainControl(method = "cv", number = 10) # cross-validation 
nnetGrid <-  expand.grid(size = seq(from = 1, to = 10, by = 1), # Hyperparameters
                        decay = seq(from = 0.1, to = 0.5, by = 0.1))

nnetFit <- train(full_time_results ~ .,  # Modelling
                 data = train_data1,
                 method = "nnet",
                 metric = "Accuracy",
                 tuneGrid = nnetGrid,
                 trControl = fitControl)
```

```{r, results = 'hide'}
p1 <-  plot(nnetFit)

pred_neural_network <-  predict(nnetFit, newdata = test_data1)
cm <- confusionMatrix(data=pred_neural_network, reference = test_data1$full_time_results)

# Store the metrics
accuracy_nnet <- cm$overall['Accuracy']
kappa_nnet <- cm$overall['Kappa']
```

After tuning the hyperparameters for our neural network model (size of `r nnetFit$bestTune$size` and weight decay of `r nnetFit$bestTune$decay`), we get a pretty good model for predicting our outcomes. An accuracy of `r cm$overall['Accuracy']` and a moderate Cohen's Kappa of `r cm$overall['Kappa']`. Compared to the previous models, neural network is the best model for now.

<br>

### Support Vector Machines

```{r, echo = TRUE, results = 'hide'}
set.seed(1)

trctrl <- trainControl(method = "cv", number=5, classProbs = T) # Cross-validation
search_grid <- expand.grid(C = c(0.01, 0.1, 1, 10, 100, 1000)) # Hyperparameters
 
svm_tuned <- train(full_time_results ~.,# Modelling
                   data = train_data1, 
                   method = "svmLinear",
                   trControl = trctrl,
                   tuneGrid = search_grid)
```

```{r, results = 'hide'}
svm.tuned.pred <- predict(svm_tuned, newdata = test_data1)
cm <- confusionMatrix(data=svm.tuned.pred, reference = test_data1$full_time_results)

# Store the metrics
accuracy_svm <- cm$overall['Accuracy']
kappa_svm <- cm$overall['Kappa']
```

We find that the best tune for C is equal to `r svm_tuned$bestTune$C`, and we obtain an accuracy of `r cm$overall['Accuracy']` and a pretty good Kappa: `r cm$overall['Kappa']`.

<br>

### Random Forests

Our last model is Random forests:

```{r, echo = TRUE, results = 'hide'}
set.seed(1)

trctrl <- trainControl(method = "cv", number=5) # Cross-validation
search_grid <- expand.grid(.mtry = c(1:15)) #Hyperparameters

rf <- train(full_time_results~., # Modelling
            data = train_data1,
            method = "rf",
            trControl = trctrl,
            tuneGrid = search_grid
)
```

```{r}
pred_rf <-  predict(rf, newdata = test_data1)
cm <- confusionMatrix(data=pred_rf, reference = test_data1$full_time_results)

# Store the metrics
accuracy_rf <- cm$overall['Accuracy']
kappa_rf <- cm$overall['Kappa']
```

For our random forest model, the hyperparameter is `r rf$bestTune$mtry` for our randomly selected predictors (aka mtry). After hypertuning our model, we got an accuracy of `r cm$overall['Accuracy']` and a Cohen's Kappa of `r cm$overall['Kappa']`.

<br>

### Summary of the models

We tried to fit several models to have the best one to predict our desired outcomes. Before moving to the next step, we need to compare each model with their respective metrics to choose the best one:

```{r}
# Create a table to show the variables
metrics = c(accuracy_nb, kappa_nb, accuracy_knn, kappa_knn, accuracy_tree, kappa_tree, accuracy_nnet, kappa_nnet, accuracy_svm, kappa_svm, accuracy_rf, kappa_rf)
cnames = c("Accuracy", "Kappa")
rnames = c("Naive Bayes", "K-nearest neighbors", "Tree", "Neural Network", "Support Vector Machines", "Random Forest")
model_output <- matrix(metrics, ncol = 2, byrow=TRUE, dimnames=list(rnames,cnames))
model_output <- model_output[order(model_output[,1],decreasing=TRUE),]

kable(model_output, digits = 2)  %>% 
  kable_styling(bootstrap_options = c("striped","condensed"), fixed_thead = T)
```

<br>

By having a look at the results, we see that Random Forest and SVM outperform the other models, followed by Neural Network.

<br>

## Variable Importance

Now that we found that our best models are Random Forest and Support Vector Machines models, we need to see if we can improve their prediction quality, that we judge by accuracy and kappa, by getting rid of the variables that do not contribute that much to the prediction. To do this, we use model-specific variable importance method.

```{r, echo = FALSE, message = FALSE, warning = FALSE, results = 'hide'}
#Creating an explain object

p_fun <- function(object, newdata){predict(object, newdata=newdata, type="prob")[,2]}

explainer_classif_rf <- DALEX::explain(rf, label = "rf",
                                       data = train_data1, 
                                       y = as.vector(as.numeric(train_data1$full_time_results)),
                                       predict_function = p_fun, 
                                       verbose = FALSE)

explainer_svm <- DALEX::explain(model = svm_tuned,
                                data = train_data1,
                                y = train_data1$full_time_results,
                                label = "SVM model")

calculate_importance <- function(your_model_explainer, n_permutations = 10) {
  imp <- model_parts(explainer = your_model_explainer,
                     B = n_permutations,
                     type = "ratio",
                     N = NULL)
  return(imp)
}

importance_rf <- calculate_importance(explainer_classif_rf)
importance_svm <- calculate_importance(explainer_svm)
```

<br>

**_Feature importance for Random forest_**

A feature is “important” if shuffling its values increases the model error, because in this case the model relied on the feature for the prediction. In our case we measure the error increase by 1-AUC (1 minus the area under the ROC curve). Features associated with a model error increase by a factor of 1. Indeed, it means that there is no change when we’re at 1 and variables were not important for predicting full time result.
In our case, variables which are not important are away corners and home corners. In opposite, the most important feature is `half_time_results` permuting `full_time_results` in an increase in 1-AUC by a factor of approximately 1.05.

<br>


```{r, echo = FALSE, message = FALSE, warning = FALSE}
plot(importance_rf)

```
<br>


By running a recursive feature elimination on the RF model, we can see that the top 5 variables (out of 18) are `full_time_results`, `half_time_results`, `away_shots_ontarget`, `half_time_home_goals` and `half_time_away_goals`. We are going to use these features in order to refit the model to see if we can improve the accuracy and Kappa.

```{r, echo=FALSE, warning=FALSE, results = 'hide'}
#Recursive feature elimination
rfeCtrl <- rfeControl(functions = rfFuncs,
                      method = "cv",
                      verbose = FALSE)

rfProfile <- rfe(x = train_data1, 
                y = as.vector(as.numeric(train_data1$full_time_results)),
                rfeControl = rfeCtrl)

rfProfile
```


In the plot below, we have the difference in entropy obtained by 10 permutations for the SVM Model. It shows graphically the average variable importance according to the cross entropy loss. Here also, there are some variables which are of high importance and which are in common with the RF model as `half_time_results` and `away_shots_ontarget` . Other crucial variables for the prediction of the result with SVM are `away_team`, `home_team` and `half_time_away_goals`. Not including theses variables can cause a decrease of the cross-entropy for SVM models.

<br>

```{r, echo = FALSE, message = FALSE, warning = FALSE}
plot(importance_svm)
```
<br>

<br>

**_Fitting the RF model with its tops feature importance_**

Once we’ve identified the most critical variables for prediction, we would like to verify that excluding all other variables increases the accuracy and kappa of our best 2 models, by refitting them.

```{r, echo=FALSE, message = FALSE, warning = FALSE}
# Random Forest (RF)
set.seed(1)
trctrl <- trainControl(method = "cv", number=5)
search_grid <- expand.grid(.mtry=rf$bestTune$mtry)
                           

VarImpRFfit <- train(full_time_results~ half_time_results + half_time_away_goals + half_time_home_goals + away_shots_ontarget, 
            data=train_data1,
            method="rf",
            trControl=trctrl,
            tuneGrid=search_grid
)

VarImpRFpred <-  predict(VarImpRFfit, newdata = test_data1)

refitconfmatrixRF <- confusionMatrix(data=VarImpRFpred, reference = test_data1$full_time_results)
oldconfmatrixRF <- confusionMatrix(data=pred_rf, reference = test_data1$full_time_results)
```

```{r, echo=FALSE,message = FALSE, warning = FALSE, out.width="100%"}
# oldconfmatrixRF$overall %>%
#   kbl(caption = "Metrics of the original model") %>%
#   kable_classic_2(full_width = F)
# refitconfmatrixRF$overall %>%
#   kbl(caption = "Metrics of the refit models")%>%
#   kable_classic_2(full_width = F)

oldaccuracy_rf <- oldconfmatrixRF$overall['Accuracy']
oldkappa_rf<- oldconfmatrixRF$overall['Kappa']

newaccuracy_rf<-  refitconfmatrixRF$overall['Accuracy']
newkappa_rf <- refitconfmatrixRF$overall['Kappa']


metrics_rf = c(oldaccuracy_rf , oldkappa_rf, newaccuracy_rf, newkappa_rf)
cnames = c("Accuracy", "Kappa") 
rnames =  c("Original RF", "Refit RF")
model_output_RF <- matrix(metrics_rf, ncol = 2, byrow=TRUE, dimnames=list(rnames,cnames))
model_output_RF <- model_output_RF[order(model_output_RF[,1],decreasing=TRUE),]

kable(model_output_RF, digits = 2)  %>% 
  kable_styling(bootstrap_options = c("striped","condensed"), fixed_thead = T)

```

<br>

When we refit the Random Forest model with the 4 most "important" variables, accuracy and kappa decrease, from `r oldconfmatrixRF$overall['Accuracy']` to `refitconfmatrixRF$overall['Accuracy']` and from `oldconfmatrixRF$overall['Kappa']` to `refitconfmatrixRF$overall['Kappa']` respectively. This reduction in accuracy and kappa metrics is not surprising, and we have a gain from reducing the complexity of the model. Hence the refitted model is still better than the original.

<br>

**_Fitting the SVM model with its tops feature importance_**

```{r, echo=FALSE,message = FALSE, warning = FALSE, out.width="50%", results = 'hide'}
# Support Vector Machine (SVM)
set.seed(1)

trctrl <- trainControl(method = "cv", number=5, classProbs = T) # Cross-validation
VarImpGrid <- expand.grid(C = svm_tuned$bestTune$C) # Hyperparameters
 
VarImpSVMfit <- train(full_time_results ~ away_team + half_time_results + home_team + half_time_away_goals + away_shots_ontarget, # Modelling
                   data = train_data1, 
                   method = "svmLinear",
                   trControl=trctrl,
                   tuneGrid = VarImpGrid)

VarImpSVM.tuned.pred <- predict(VarImpSVMfit, newdata = test_data1)

```

```{r, echo=FALSE}
oldconfmatrixSVM <-confusionMatrix(data=svm.tuned.pred, reference = test_data1$full_time_results)
refitconfmatrixSVM <- confusionMatrix(data=VarImpSVM.tuned.pred, reference = test_data1$full_time_results)

# oldconfmatrixSVM$overall %>%
#   kbl(caption = "Metrics of the original model") %>%
#   kable_classic_2(full_width = F)
# refitconfmatrixSVM$overall %>%
#   kbl(caption = "Metrics of the refit models")%>%
#   kable_classic_2(full_width = F)

oldaccuracy_svm <- oldconfmatrixSVM$overall['Accuracy']
oldkappa_svm<- oldconfmatrixSVM$overall['Kappa']

newaccuracy_svm <-  refitconfmatrixSVM$overall['Accuracy']
newkappa_svm <- refitconfmatrixSVM$overall['Kappa']


metrics_svm = c(oldaccuracy_svm , oldkappa_svm, newaccuracy_svm, newkappa_svm)
cnames = c("Accuracy", "Kappa") 
rnames =  c("Original SVM ", "Refit SVM")
model_output_svm <- matrix(metrics_svm, ncol = 2, byrow=TRUE, dimnames=list(rnames,cnames))
model_output_svm <- model_output_svm[order(model_output_svm[,1],decreasing=TRUE),]

kable(model_output_svm, digits = 2)  %>% 
  kable_styling(bootstrap_options = c("striped","condensed"), fixed_thead = T)

```

<br>

For the Support Vector Machine model with the 5 most "important" variables, which is : `away_team`, `half_time_results`, `home_team`, `half_time_away_goals` and `away_shots_ontarget`. The accuracy doesn't change, we have `r oldconfmatrixSVM$overall['Accuracy']`  which is a really good sign, and kappa slightly decrease from `r oldconfmatrixSVM$overall['Kappa']` to `r refitconfmatrixSVM$overall['Kappa']`. This refit is reducing the complexity of our model since we are taking less variables and even though we have a lower Cohen's Kappa, this changes is marginal. Therefore, it is wiser to use the refitted model to make predictions.


