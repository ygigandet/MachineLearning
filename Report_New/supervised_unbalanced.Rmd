# Supervised Learning

```{r, echo = FALSE, message = FALSE}
source(here::here("scripts/setup.R"))
```

```{r, echo = FALSE, message = FALSE, warning = FALSE}
#  Load the cleaned dataset from the data.Rmd
setwd(here::here("Data_New/"))
premier_league_results <- read_csv("premier_league_results_clean.csv")
```

Next step is fitting models to the clean data, based on our EDA outcomes. But before, to avoid overfitting, we need to split our data into a training and test set.

## Data Splitting
Data splitting is when data is divided into two or more subsets. Typically, with a two-part split, one part is used to evaluate or test the data and the other to train the model. Splitting into Training and test set help to avoid overfitting also without proper data, machine learning models are hard to implement and can lead to completely different conclusions and interpretation. To achieve our aim, the strategy which we use is Cross validation (CV) data splitting method. Since we have a large amount of data, the cross validation method is best one for our dataset, indeed it might be problematic if we donâ€™t have enough data and might choose bootstrapping. 

Moreover, it is not necessary to balance the training set, because we are in a non-binary case, and the interpretation of the results will be more complex and the accuracy of the models is decreasing. For these reasons, we decided to keep original values without balancing.

Finally, We performed a random train and test split of our data in order to use the following models: CART, KNN, Naive Bayes, SVM, Neural Network.

```{r}
# Just removing the match data, full_time_home_goals and full_time_away_goals

premier_league_results <- premier_league_results %>% 
  select(-match_date, -full_time_home_goals, -full_time_away_goals)

# Need to factorize some variables - namely the characters
premier_league_results$full_time_results = as.factor(premier_league_results$full_time_results)
premier_league_results$half_time_results = as.factor(premier_league_results$half_time_results)
premier_league_results$home_team = as.factor(premier_league_results$home_team)
premier_league_results$away_team = as.factor(premier_league_results$away_team)

# Set the seeds to have the same samples over and over

set.seed(234)

train_indexes1 = createDataPartition(premier_league_results$full_time_results,p=0.8,list=FALSE)

train_data1 = premier_league_results[train_indexes1,]

test_data1 = premier_league_results[-train_indexes1,]
```

## Model fitting

Since we're trying to predict whether the result of a football match is either a win for the home team, a win for the away team or a draw, our models should be adapted to the case of a multiclass classification task. In this next part, we will fit multiple models and choose at the end the best 2 models, according to their score of accuracy and Cohen's Kappa metrics. This means that we are looking for models with high accuracy, meaning that the predicted variables match the observed variables as much as possible, and a high Cohen's Kappa, which is an indicator whether the model in hand is a better/different one than a random one.

### Naive Bayes

```{r}
set.seed(1)

naivebayesfit <- naive_bayes(full_time_results~., data = train_data1, usekernel=TRUE)

naivebayespred <- predict(naivebayesfit, newdata = test_data1, type="class")

confusionMatrix(data=as.factor(naivebayespred), reference = test_data1$full_time_results)
```

For our first model, the accuracy is at 0.667 and the Kappa at 0.422. 

### K-NN

```{r}
set.seed(1)

#Tuning hyperparameter
ctrl <- trainControl(method = "cv", number=10)
knngrid <- train(full_time_results ~., data = train_data1, method = "knn", trControl=ctrl, tuneGrid = data.frame(k=c(1,2,5,10,50,100,125,150)))

knngrid
```

```{r}
knnfit <- knn3(full_time_results ~ ., data=train_data1, k=125)

knnpred <- predict(knnfit, newdata = test_data1, type="class")

confusionMatrix(data=as.factor(knnpred), reference = test_data1$full_time_results)
```

After hypertuning the k parameter, AKA the number of closest neighbors, we fitted model with the optimal parameter k=125, gives a convenient prediction. We have an accuracy of 0.574 and a low Kappa : 0.227. This result is worse than the Naive Bayes model.


### Classification Tree

```{r}
set.seed(1)

#Tuning hyperparameter
ctrl <- trainControl(method = "cv", number=10)
treegrid <- train(full_time_results ~., data = train_data1, method = "rpart", trControl=ctrl, tuneGrid = data.frame(cp=c(0.005,0.01,0.015,0.02,0.025,0.03,0.35)))

treegrid
```

```{r}
classtreefit <- rpart(full_time_results ~ ., data=train_data1)

rpart.plot(classtreefit)

plotcp(classtreefit)
classtreefit.pruned <- prune(classtreefit, cp=0.015)

classtreepred <- predict(classtreefit.pruned, newdata=test_data1, type="class")

confusionMatrix(data=as.factor(classtreepred), reference = test_data1$full_time_results)
```

After finding the optimal cp= 0.015 with hypertuning for pruning, the CART model gives a convenient prediction. We've got an accuracy of 0.639 and a moderate Kappa : 0.413. The performance of this model is actually closer to the Naive Bayes !

### Neural Networks

```{r}
# Now that our sets are ready, let's apply the neural network with caret
set.seed(1)

fitControl <- trainControl(method = "cv", # cv stands for cross-validation 
                           number = 10)

nnetGrid <-  expand.grid(size = seq(from = 1, to = 10, by = 1),
                        decay = seq(from = 0.1, to = 0.5, by = 0.1))

nnetFit <- train(full_time_results ~ ., 
                 data = train_data1,
                 method = "nnet",
                 metric = "Accuracy",
                 tuneGrid = nnetGrid,
                 trControl = fitControl)

p1 <-  plot(nnetFit)
p1

pred_neural_network <-  predict(nnetFit, newdata = test_data1)
confusionMatrix(data=pred_neural_network, reference = test_data1$full_time_results)
```

After tuning the hyperparameters for our neural network model (size of 1 and weight decay of 0.1), we get a pretty good model for predicting our outcomes. An accuracy of 0.685 and a moderate Cohen's Kappa of 0.457. Compared to the previous models, neural network provides the best model.

## Support Vector Machines

The SVM model gives a convenient prediction, we've an accuracy of 0.648 and a pretty low Kappa: 0.373.

```{r}
# Tuning the hyperparameters
trctrl <- trainControl(method = "cv", number=5)
grid <- expand.grid(C = c(0.01, 0.1, 1, 10, 100, 1000))

set.seed(143)
svm_Linear_Grid <- train(full_time_results ~., 
                         data = train_data1, 
                         method = "svmLinear",
                         trControl=trctrl,
                         tuneGrid = grid)

svm_Linear_Grid
plot(svm_Linear_Grid)
svm_Linear_Grid$bestTune
```
```{r}
# Now that we know the tuning hyperparameters, let's apply it

svm_tuned <- svm(full_time_results~., data = train_data1,
                 kernel = "linear",
                 cost = svm_Linear_Grid$bestTune$C)


svm.tuned.pred <- predict(svm_tuned, newdata = test_data1)
confusionMatrix(data=svm.tuned.pred, reference = test_data1$full_time_results)
```

## Findings


