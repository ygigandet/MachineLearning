# Supervised Learning

```{r, echo = FALSE, message = FALSE}
source(here::here("scripts/setup.R"))
```

```{r, echo = FALSE, message = FALSE, warning = FALSE}
#  Load the cleaned dataset from the data.Rmd
setwd(here::here("Data_New/"))
premier_league_results <- read_csv("premier_league_results_clean.csv")
```

Next step is fitting models to the clean data, based on our EDA outcomes. But before, to avoid overfitting, we need to split our data into a training and test set.

## Data Splitting
Data splitting is when data is divided into two or more subsets. Typically, with a two-part split, one part is used to evaluate or test the data and the other to train the model. Splitting into Training and test set help to avoid overfitting also without proper data, machine learning models are hard to implement and can lead to completely different conclusions and interpretation. To achieve our aim, the strategy which we use is Cross validation (CV) data splitting method. Since we have a large amount of data, the cross validation method is best one for our dataset, indeed it might be problematic if we don’t have enough data and might choose bootstrapping. 

Moreover, it is not necessary to balance the training set, because we are in a non-binary case, and the interpretation of the results will be more complex and the accuracy of the models is decreasing. For these reasons, we decided to keep original values without balancing.

Finally, We performed a random train and test split of our data in order to use the following models: CART, KNN, Naive Bayes, SVM, Neural Network.

```{r}
# Just removing the match data, full_time_home_goals and full_time_away_goals

premier_league_results <- premier_league_results %>% 
  select(-match_date, -full_time_home_goals, -full_time_away_goals)

# Need to factorize some variables - namely the characters
premier_league_results$full_time_results = as.factor(premier_league_results$full_time_results)
premier_league_results$half_time_results = as.factor(premier_league_results$half_time_results)
premier_league_results$home_team = as.factor(premier_league_results$home_team)
premier_league_results$away_team = as.factor(premier_league_results$away_team)

# Set the seeds to have the same samples over and over

set.seed(234)

train_indexes1 = createDataPartition(premier_league_results$full_time_results,p=0.8,list=FALSE)

train_data1 = premier_league_results[train_indexes1,]

test_data1 = premier_league_results[-train_indexes1,]
```

## Model fitting

Since we're trying to predict whether the result of a football match is either a win for the home team, a win for the away team or a draw, our models should be adapted to the case of a multiclass classification task. In this next part, we will fit multiple models and choose at the end the best 2 models, according to their score of accuracy and Cohen's Kappa metrics. This means that we are looking for models with high accuracy, meaning that the predicted variables match the observed variables as much as possible, and a high Cohen's Kappa, which is an indicator whether the model in hand is a better/different one than a random one.

### Naive Bayes

```{r, warning = FALSE}
set.seed(1)

trctrl <- trainControl(method = "cv", number=10)

search_grid <- expand.grid(
  usekernel = c(TRUE, FALSE),
  laplace = c(1:5),
  adjust = seq(0, 5, by = 1)
)

naivebayesfit <- train(full_time_results ~.,
                 data = train_data1,
                 method = "naive_bayes",
                 trControl = trctrl,
                 tuneGrid = search_grid)

naivebayespred <- predict(naivebayesfit, newdata = test_data1)

confusionMatrix(data=as.factor(naivebayespred), reference = test_data1$full_time_results)
```

For our first model, the accuracy is at 0.667 and the Kappa at 0.422. 

### K-NN

```{r}
set.seed(1)

#Tuning hyperparameter
ctrl <- trainControl(method = "cv", number=10)
knngrid <- train(full_time_results ~., data = train_data1, method = "knn", trControl=ctrl, tuneGrid = data.frame(k=c(1,2,5,10,50,100,125,150)))

knngrid
```

```{r}
knnfit <- knn3(full_time_results ~ ., data=train_data1, k=125)

knnpred <- predict(knnfit, newdata = test_data1, type="class")

confusionMatrix(data=as.factor(knnpred), reference = test_data1$full_time_results)
```

After hypertuning the k parameter, AKA the number of closest neighbors, we fitted model with the optimal parameter k=125, gives a convenient prediction. We have an accuracy of 0.574 and a low Kappa : 0.227. This result is worse than the Naive Bayes model.


### Classification Tree

```{r}
set.seed(1)

#Tuning hyperparameter
ctrl <- trainControl(method = "cv", number=10)
treegrid <- train(full_time_results ~., data = train_data1, method = "rpart", trControl=ctrl, tuneGrid = data.frame(cp=c(0.005,0.01,0.015,0.02,0.025,0.03,0.35)))

treegrid
```

```{r}
classtreefit <- rpart(full_time_results ~ ., data=train_data1)

rpart.plot(classtreefit)

plotcp(classtreefit)
classtreefit.pruned <- prune(classtreefit, cp=0.015)

classtreepred <- predict(classtreefit.pruned, newdata=test_data1, type="class")

confusionMatrix(data=as.factor(classtreepred), reference = test_data1$full_time_results)
```

After finding the optimal cp= 0.015 with hypertuning for pruning, the CART model gives a convenient prediction. We've got an accuracy of 0.639 and a moderate Kappa : 0.413. The performance of this model is actually closer to the Naive Bayes !

### Neural Networks

```{r}
# Now that our sets are ready, let's apply the neural network with caret
set.seed(1)

fitControl <- trainControl(method = "cv", # cv stands for cross-validation 
                           number = 10)

nnetGrid <-  expand.grid(size = seq(from = 1, to = 10, by = 1),
                        decay = seq(from = 0.1, to = 0.5, by = 0.1))

nnetFit <- train(full_time_results ~ ., 
                 data = train_data1,
                 method = "nnet",
                 metric = "Accuracy",
                 tuneGrid = nnetGrid,
                 trControl = fitControl)

p1 <-  plot(nnetFit)
p1

pred_neural_network <-  predict(nnetFit, newdata = test_data1)
confusionMatrix(data=pred_neural_network, reference = test_data1$full_time_results)
```

After tuning the hyperparameters for our neural network model (size of 1 and weight decay of 0.1), we get a pretty good model for predicting our outcomes. An accuracy of 0.685 and a moderate Cohen's Kappa of 0.457. Compared to the previous models, neural network provides the best model.

## Support Vector Machines

The SVM model gives a convenient prediction, we've an accuracy of 0.648 and a pretty low Kappa: 0.373.

```{r}
# Tuning the hyperparameters
trctrl <- trainControl(method = "cv", number=5)
grid <- expand.grid(C = c(0.01, 0.1, 1, 10, 100, 1000))

set.seed(143)
svm_Linear_Grid <- train(full_time_results ~., 
                         data = train_data1, 
                         method = "svmLinear",
                         trControl=trctrl,
                         tuneGrid = grid)

svm_Linear_Grid
plot(svm_Linear_Grid)
svm_Linear_Grid$bestTune
```
```{r}
# Now that we know the tuning hyperparameters, let's apply it

svm_tuned <- svm(full_time_results~., data = train_data1,
                 kernel = "linear",
                 cost = svm_Linear_Grid$bestTune$C)


svm.tuned.pred <- predict(svm_tuned, newdata = test_data1)
confusionMatrix(data=svm.tuned.pred, reference = test_data1$full_time_results)
```

## Findings


# Variable of Importance


Variable importance is a method that provides a measure of the importance of each feature for the model prediction quality. It helps to analyze the link between the features and the response, through the model predictions. 
Our variable importance is model-specific since the approaches that are specific to the model that is used. We have apply variable of importance on our 3 best models: NN, CART, NB. To do so, we create an explain object with DALEX :: explain function, then implement a variable importance calculation. 

Studies show that a higher score means that the specific feature will have a larger effect on the model that is being used to predict a certain variable. Based on this principle, we can see that either in CART or NN, half time results is an important variable. Indeed, a change on this variable can drastically change the result of both models. Then, we have half time away goal followed by away shoot on target.
For each feature, the difference in entropy obtained by 10 permutations is shown by the following plot. It shows graphically the average importance variable according to the cross entropy loss.

```{r, echo = FALSE, message = FALSE, warning = FALSE}
# Take the NN machine model 
nnetFit

# Take the CART machine model 
classtreefit.pruned

# Take the NB machine model 
naivebayesfit

#Creating an explain object
x_train <- select(train_data1, -full_time_results)
y_train <- pull(train_data1, full_time_results)

explainer_nn <- DALEX::explain(model = nnetFit,
                                is_multiclass = TRUE,
                                data = x_train,
                                y = y_train,
                                label = "NN machine model")
  #NN fonctionne

explainer_cart <- DALEX::explain(model = classtreefit.pruned,
                                is_multiclass = TRUE,
                                data = x_train,
                                y = y_train,
                                label = "CART machine model ")
  #CART fonctionne

explainer_nb <- DALEX::explain(model = naivebayesfit,
                                is_multiclass = TRUE,
                                data = x_train,
                                y = as.numeric(y_train),
                                label = "NB machine model")
```

# Plotting the feature importance
```{r, echo = FALSE, message = FALSE, warning = FALSE}
calculate_importance <- function(your_model_explainer, n_permutations = 10) {
  imp <- model_parts(explainer = your_model_explainer,
                     B = n_permutations,
                     type = "ratio",
                     N = NULL)
  return(imp)
}

importance_nn <- calculate_importance(explainer_nn)
importance_cart <- calculate_importance(explainer_cart)
importance_nb <- calculate_importance(explainer_nb)

plot(importance_nn, importance_cart) +
 ggtitle("Mean variable-importance ratio over 10 permutations", "")

```
We know that smaller the entropy is, better is the model. In term of cross entropy, ....

## We might not keep it, depend on the space and utility
Once we’ve identified influential variables across all three models, next we likely want to understand how the relationship between these influential variables and the average prediction response differ between the models.

To do so, we can run a partial dependence profile

```{r}
#
adp_nn <- model_profile(explainer_nn, 
 variables = "half_time_results")
plot(adp_nn)

adp_cart <- model_profile(explainer_cart, 
 variables = "half_time_results")
plot(adp_cart)

#PLot 
plot(adp_nn$agr_profiles, 
 adp_cart$agr_profiles)

plot(adp_nn, adp_cart) +
 ggtitle("Partial dependence profile", "")

```

#Refitting the model with the 5 most important variables

```{r, echo=FALSE}
# Naive Bayes --> 

set.seed(1)
trctrl <- trainControl(method = "cv", number=10)
search_grid <- expand.grid(
  usekernel = c(TRUE, FALSE),
  laplace = c(1:5),
  adjust = seq(0, 5, by = 1)
)
VarImpNBfit <- train(full_time_results ~ half_time_results + half_time_away_goals + away_shots_ontarget + half_time_home_goals,
                 data = train_data1,
                 method = "naive_bayes",
                 trControl = trctrl,
                 tuneGrid = search_grid)

VarImpNBpred <- predict(VarImpNBfit, newdata = test_data1, type="class")
```

```{r, echo=FALSE}
#Comparaison between the old cconufsion matrix and the refit confusion matrix 
refitconfmatrixNB <- confusionMatrix(data=as.factor(VarImpNBpred), reference = test_data1$full_time_results)

```

```{r, echo=FALSE}
# NN
nnetFit <- train(full_time_results ~ half_time_results + half_time_away_goals + away_shots_ontarget + half_time_home_goals,
                 data = train_data1,
                 method = "nnet",
                 metric = "Accuracy",
                 tuneGrid = nnetGrid,
                 trControl = fitControl)

pred_neural_network <-  predict(nnetFit, newdata = test_data1)
```

```{r, echo=FALSE}
oldconfmatrixNN <-confusionMatrix(data=pred_neural_network, reference = test_data1$full_time_results)

refitconfmatrixNN <- confusionMatrix(data=pred_neural_network, reference = test_data1$full_time_results)

grid.arrange(oldconfmatrixNN, refitconfmatrixNN)
```

```{r, echo=FALSE}
# CART
classtreefit <- rpart(full_time_results ~ half_time_results + half_time_away_goals + away_shots_ontarget + half_time_home_goals,, data=train_data1)

rpart.plot(classtreefit)

plotcp(classtreefit)
classtreefit.pruned <- prune(classtreefit, cp=0.015)

classtreepred <- predict(classtreefit.pruned, newdata=test_data1, type="class")
```

```{r, echo=FALSE}

refitconfmatrixCart <- confusionMatrix(data=as.factor(classtreepred), reference = test_data1$full_time_results)

```