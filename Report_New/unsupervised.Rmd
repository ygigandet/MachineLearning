# Unsupervised Learning

```{r, echo = FALSE, message = FALSE}
source(here::here("scripts/setup.R"))
```

## Clustering

```{r, echo=FALSE}
library(gridExtra)
library(readr)
library(dplyr)
library(FactoMineR)
library(factoextra)
library(ggplot2)
library(reshape2)

setwd(here::here("Data_New/"))

premier_league_results <- read_csv("premier_league_results_clean.csv")

head(premier_league_results)
summary(premier_league_results)

#select the numeric variables
PLR.numeric <- premier_league_results %>% dplyr::select(where(is.numeric))

## scale all the features
PLR.numeric.scale <- scale(PLR.numeric[,]) 

# matrix of Manhattan distances 
PLR.d <- dist(PLR.numeric.scale[,], method = "manhattan") 

# create a data frame of the distances in long format
PLR.melt <- melt(as.matrix(PLR.d))

head(PLR.melt)

ggplot(data = PLR.melt, aes(x=Var1, y=Var2, fill=value)) + 
  geom_tile()

#dendrogram using a complete linkage

PLR.hc <- hclust(PLR.d, method = "complete")
plot(PLR.hc, hang=-1)

#We cut the tree to 4 clusters, and represent the result.
#We also extract the cluster assignment of each wine.

plot(PLR.hc, hang=-1)
rect.hclust(PLR.hc, k=4)
PLR.clust <- cutree(PLR.hc, k=4)
PLR.clust


fviz_nbclust(PLR.numeric.scale,
             hcut, hc_method="complete",
             hc_metric="manhattan",
             method = "wss", 
             k.max = 25, verbose = FALSE)
fviz_nbclust(PLR.numeric.scale,
             hcut, hc_method="complete",
             hc_metric="manhattan",
             method = "silhouette", 
             k.max = 25, verbose = FALSE)
fviz_nbclust(PLR.numeric.scale,
             hcut, hc_method="complete",
             hc_metric="manhattan",
             method = "gap", 
             k.max = 25, verbose = FALSE)
#K-means
fviz_nbclust(PLR.numeric.scale,
             kmeans,
             method = "wss", 
             k.max = 25, verbose = FALSE)
fviz_nbclust(PLR.numeric.scale,
             kmeans, 
             method = "silhouette", 
             k.max = 25, verbose = FALSE)
fviz_nbclust(PLR.numeric.scale,
             kmeans,
             method = "gap", 
             k.max = 25, verbose = FALSE)

PLR.km <- kmeans(PLR.numeric.scale, centers=2)
PLR.km$cluster
```


## PCA

```{r, echo=FALSE}
PLR.numeric <- premier_league_results %>% dplyr::select(where(is.numeric))
PLR.pca <- PCA(PLR.numeric, ncp = 11, graph = FALSE)

PLR.pca
```

```{r, echo=FALSE}
fviz_pca_var(PLR.pca)
```


```{r, echo=FALSE}
# Extract the contributions of each features in the dimension.
fviz_contrib(PLR.pca, choice = "var", axes = 1) #aes determines which dimension
```

```{r, echo=FALSE}
fviz_pca_ind(PLR.pca) ## only the individuals
fviz_pca_biplot(PLR.pca) ## biplot
```


```{r, echo=FALSE}
fviz_eig(PLR.pca, addlabels = TRUE, ncp=11)
```


```{r, echo=FALSE}
p1 <- fviz_pca_biplot(PLR.pca, axes = 1:2) 
p2 <- fviz_pca_biplot(PLR.pca, axes = 3:4) 
p3 <- fviz_pca_biplot(PLR.pca, axes = 5:6) 
grid.arrange(p1, p2, p3, nrow = 2, ncol=2)
```

```{r, echo=FALSE}
#to be fixed
PLR.hc <- hclust(dist(premier_league_results, method = "manhattan"))
PLR.clust <- cutree(PLR.hc, k = 6)
fviz_pca_biplot(PLR.pca,
             col.ind = factor(PLR.clust))
```

## Unsupervised Learning Findings

xxx