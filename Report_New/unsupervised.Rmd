# Unsupervised Learning

```{r, echo = FALSE, message = FALSE}
source(here::here("scripts/setup.R"))
```


In order to have a clearer view on the data, we decided to split the main dataset to two datasets; one for Home teams and another for Away teams. The goal is to study the performance of each team when they are both away or home. 


```{r, echo=FALSE, warning=FALSE}
setwd(here::here("Data_New/"))
premier_league_results <- read_csv("premier_league_results_clean.csv")
```

```{r, echo=FALSE, include=FALSE}
# Divide the dataset into Home and Away

PLR_Home <- premier_league_results %>% select(home_team, full_time_home_goals, half_time_home_goals, home_shots, home_shots_ontarget, home_fouls, home_corners, home_yellow_cards, home_red_cards)

PLR_Away <- premier_league_results %>% select (away_team, full_time_away_goals, half_time_away_goals, away_shots, away_shots_ontarget, away_fouls, away_corners, away_yellow_cards, away_red_cards)

head(PLR_Home) %>% kable(caption = "**Premiere League Results for Home Teams**") %>%
  kable_styling(bootstrap_options = c("striped","hover","condensed"),
                fixed_thead = T) %>% kable_paper() %>%  scroll_box(width = "100%", height = "300px")
```

```{r, echo=FALSE, include=FALSE}
head(PLR_Away) %>% kable(caption = "**Premiere League Results for Away Teams**") %>%
  kable_styling(bootstrap_options = c("striped","hover","condensed"),
                fixed_thead = T) %>% kable_paper() %>%  scroll_box(width = "100%", height = "300px")
```

To have a proper overview of the variables for each team, we have grouped by the teams calculating the median of each variable to obtain a robust grouping. 
After performing the grouping, our dataset was consisting of 14 instances pertaining to the 14 chosen teams and a total of 9 variables which includes the `team name` character column. 
```{r, echo=FALSE, include=FALSE}
# Group by teams

PLR_Home <- PLR_Home %>% group_by(home_team) %>% 
  summarise(Full_time_home_goals = median(full_time_home_goals), 
            Half_time_home_goals = median(half_time_home_goals), 
            Home_shots = median(home_shots),
            Home_shots_ontarget = median(home_shots_ontarget),
            Home_fouls = median(home_fouls),
            Home_corners = median(home_corners),
            Home_yellow_cards = median(home_yellow_cards),
            Home_red_cards = mean(home_red_cards), .groups = 'drop')


PLR_Away <- PLR_Away %>% group_by(away_team) %>% 
  summarise(Full_time_away_goals = median(full_time_away_goals), 
            Half_time_away_goals = median(half_time_away_goals), 
            Away_shots = median(away_shots),
            Away_shots_ontarget = median(away_shots_ontarget),
            Away_fouls = median(away_fouls),
            Away_corners = median(away_corners),
            Away_yellow_cards = median(away_yellow_cards),
            Away_red_cards = mean(away_red_cards), .groups = 'drop')
```

```{r, echo=FALSE, include=FALSE}
head(PLR_Home)%>% kable(caption = "**Median Premiere League Results for Home Teams**") %>%
  kable_styling(bootstrap_options = c("striped","hover","condensed"),
                fixed_thead = T) %>% kable_paper() %>%  scroll_box(width = "100%", height = "300px")
```

```{r, echo=FALSE, include=FALSE}
head(PLR_Away)%>% kable(caption = "**Median Premiere League Results for Away Teams**") %>%
  kable_styling(bootstrap_options = c("striped","hover","condensed"),
                fixed_thead = T) %>% kable_paper() %>%  scroll_box(width = "100%", height = "300px")
```

Our dataset contains eight (8) numeric variables, each of which has its own scale. As we can see below, the "Home_shots" variable has a scale ranging from around 10 to 18, while "Full_time_home_goals" has a scale ranging from around 0 to 3. Therefore, we will need to scale the data in order to give the same weight for all variables. 
```{r, echo=FALSE}

# Scale features for home and away

PLR_Home_Scale <- data.frame(scale(PLR_Home[,-1], center=FALSE)) # %>% as_data_frame()
PLR_Away_Scale <- data.frame(scale(PLR_Away[,-1], center=FALSE)) # %>% as_data_frame()


PLR_Home_Scale <- PLR_Home_Scale %>% 
  mutate(home_team = PLR_Home$home_team, .before = Full_time_home_goals )

PLR_Away_Scale <- PLR_Away_Scale %>% 
  mutate(away_team = PLR_Away$away_team, .before = Full_time_away_goals )
```

The following two tables show the scaled datasets for the first 6 teams. 

```{r, echo=FALSE}
head(PLR_Home_Scale)%>% kable(caption = "**Scaled Premiere League Results for Home Teams**") %>%
  kable_styling(bootstrap_options = c("striped","hover","condensed"),
                fixed_thead = T) %>% kable_paper() %>%  scroll_box(width = "100%", height = "220px")
```
<br>

```{r, echo=FALSE}
head(PLR_Away_Scale)%>% kable(caption = "**Scaled Premiere League Results for Away Teams**") %>%
  kable_styling(bootstrap_options = c("striped","hover","condensed"),
                fixed_thead = T) %>% kable_paper() %>%  scroll_box(width = "100%", height = "220px")
```
<br>
<br>

## Principal Component Analysis (PCA)

For our unsupervised learning model, we decided to work on PCA in order to see the clusters formed and study the important components of each of the Home and Away teams. 

```{r, echo=FALSE}
rownames(PLR_Home_Scale)[1] <- "Arsenal"
rownames(PLR_Home_Scale)[2] <- "Bournemouth"
rownames(PLR_Home_Scale)[3] <- "Burnley"
rownames(PLR_Home_Scale)[4] <- "Chelsea"
rownames(PLR_Home_Scale)[5] <- "Crystal Palace"
rownames(PLR_Home_Scale)[6] <- "Everton"
rownames(PLR_Home_Scale)[7] <- "Leicester"
rownames(PLR_Home_Scale)[8] <- "Liverpool"
rownames(PLR_Home_Scale)[9] <- "Man City"
rownames(PLR_Home_Scale)[10] <- "Man United"
rownames(PLR_Home_Scale)[11] <- "Southampton"
rownames(PLR_Home_Scale)[12] <- "Tottenham"
rownames(PLR_Home_Scale)[13] <- "Watford"
rownames(PLR_Home_Scale)[14] <- "West Ham"

rownames(PLR_Away_Scale)[1] <- "Arsenal"
rownames(PLR_Away_Scale)[2] <- "Bournemouth"
rownames(PLR_Away_Scale)[3] <- "Burnley"
rownames(PLR_Away_Scale)[4] <- "Chelsea"
rownames(PLR_Away_Scale)[5] <- "Crystal Palace"
rownames(PLR_Away_Scale)[6] <- "Everton"
rownames(PLR_Away_Scale)[7] <- "Leicester"
rownames(PLR_Away_Scale)[8] <- "Liverpool"
rownames(PLR_Away_Scale)[9] <- "Man City"
rownames(PLR_Away_Scale)[10] <- "Man United"
rownames(PLR_Away_Scale)[11] <- "Southampton"
rownames(PLR_Away_Scale)[12] <- "Tottenham"
rownames(PLR_Away_Scale)[13] <- "Watford"
rownames(PLR_Away_Scale)[14] <- "West Ham"
```


```{r, echo=FALSE, include=FALSE}
#PLR_Home_numeric <- PLR_Home_Scale %>% select(where(is.numeric))
#PLR_Away_numeric <- PLR_Away_Scale %>% select(where(is.numeric)) 

PLR_Home_pca <- PCA(PLR_Home_Scale [,-1], ncp = 11, graph = FALSE)
PLR_Away_pca <- PCA(PLR_Away_Scale [,-1], ncp = 11, graph = FALSE)


PLR_Home_pca

#PCAmix(X.quanti = NULL, X.quali = NULL, ndim = 5, rename.level = FALSE, weight.col.quanti = NULL, weight.col.quali = NULL, graph = TRUE)
```


```{r, echo=FALSE, include=FALSE}
PLR_Away_pca
```

<br>

**_Graphing of Variables_**

The below two graphs show the same eight (8) variables and for all of the teams when they are both Home and Away. Dimension 1 explains 54.3% of the variance of the data and Dimension 2 explains 21.4% making a total of 75.7% of the variance explained by these two components.  
Both graphs make sense since they show that all variables related to "goals" and "shots" are correlated to Dimension 2 and variables related to "cards" are correlated with Dimension 1. Therefore, we can deduce that Dimension 1 relates to Goals and Dimension 2 related to Cards.                      

```{r, echo=FALSE}

# fviz_pca_var(PLR_Home_pca, repel = TRUE, title = "Variable PCA for Home Teams")
# fviz_pca_var(PLR_Away_pca, repel = TRUE, title = "Variable PCA for Away Teams")

grid.arrange(fviz_pca_var(PLR_Home_pca, repel = TRUE, title = "Home Teams", col.var = "Blue"), fviz_pca_var(PLR_Away_pca, repel = TRUE, title = "Away Teams", col.var = "#2EDFC7"), ncol=2)
```
**_Contribution of Variables_**

To  better interpret these dimensions, we can extract the contributions of each features in the dimension. Below, for Dimension1. We can see below the graphs showing contributions of each variable in both Home and Away states. We can conclude that whether the teams are playing Home or Away, there isn't a huge change in the contribution of the variables. 

Further, as shown in the variable graphs, the variables _yellow cards_, _red cards_, and _fouls_ are correlated with Dimension 2. 

```{r, echo=FALSE}
# Extract the contributions of each features in the dimension.
fviz_contrib(PLR_Home_pca, choice = "var", axes = 1, title="Variable Contribution to Dim-1") + fviz_contrib(PLR_Away_pca, choice = "var", axes = 1, title="Variable Contribution to Dim-1", fill = "#2EDFC7")

fviz_contrib(PLR_Home_pca, choice = "var", axes = 2, title="Variable Contribution to Dim-2") + fviz_contrib(PLR_Away_pca, choice = "var", axes = 2, title="Variable Contribution to Dim-2", fill = "#2EDFC7")
```

```{r, echo=FALSE}
# fviz_pca_ind(PLR_Home_pca) ## only the individuals
# fviz_pca_ind(PLR_Away_pca)

# fviz_pca_biplot(PLR_Home_pca, repel = TRUE, col.var = "#2E9FDF", col.ind = "#696969", label = "all")
```

<br>
```{r, echo=FALSE}
# fviz_pca_biplot(PLR_Away_pca, repel = TRUE, col.var = "#2EDFC7", col.ind = "#696969", label = "all")
```
**_Dimension Analysis_**

The summaries below show percentage of contribution of each principle component and the total percentage explained along with various details of each component and individual. The scree plots below show an easier and graphical representation of the same percentage of variance for all the Dimensions. As shown below, the first 3 components explain more than 80% of the variance. Given that even the first two (2) components explain more than 70% of the variance is also good enough. 

<br>
<br>
Summary of Variables - Home: 
```{r, echo=FALSE}
PLR_Home_pca$eig %>% kable(caption = "Summary of Variables - Home") %>%
  kable_styling(bootstrap_options = c("striped","hover","condensed"),
                fixed_thead = T) %>% kable_paper() %>%  scroll_box(width = "100%", height = "200px")
```
<br>
<br>
Summary of Variables - Away: 
```{r, echo=FALSE}
PLR_Away_pca$eig %>% kable(caption = "Summary of Variables - Away") %>%
  kable_styling(bootstrap_options = c("striped","hover","condensed"),
                fixed_thead = T) %>% kable_paper() %>%  scroll_box(width = "100%", height = "200px")
```

<br>
<br>

```{r, echo=FALSE}
ScreeHome <- fviz_eig(PLR_Home_pca, addlabels = TRUE, ncp=11, title ="Home - Scree plot" )
ScreeAway <- fviz_eig(PLR_Away_pca, addlabels = TRUE, ncp=11, title ="Away - Scree plot" )

ScreeHome + ScreeAway

```

<br>
<br>
**_Biplots_**

Creating the biplots below, we can combine clustering and pca in order to see each individual, it's relationship with other individuals and variables and create clusters. We have decided to create three (3) clusters to decrease the _within variance_ and increase the _between variance_  keeping in mind the complexity of the model, we have created three clusters. 

We can see that, for both Home and Away graphs, the clusters represented are separated by the strength of correlation with each Dimension.

```{r, echo=FALSE}
PLR_Home_hc2 <- hclust(dist(PLR_Home_Scale[,-1], method = "manhattan"), method = "complete")
PLR_Away_hc2 <- hclust(dist(PLR_Away_Scale[,-1], method = "manhattan"), method = "complete")

PLR_Home_clust <- cutree(PLR_Home_hc2, k = 3)
PLR_Away_clust <- cutree(PLR_Away_hc2, k = 3)

fviz_pca_biplot(PLR_Home_pca,
             col.ind = factor(PLR_Home_clust),  
             #addEllipses = TRUE, 
             #ellipse.type = "confidence",
             legend.title = "Clusters", 
             repel = TRUE)
```

<br>
When the team is playing in "Home", the three clusters are distinguished by the following: 

    
  + `Cluster1` - Consists of _Man United_ and _Arsenal_; both of which have relatively a weak positive correlation to Dimension 2 and strong positive correlation to Dimension 1. This means that they relatively have more cards than other teams along with a good number of goals. 
  + `Cluster2` - This is the largest cluster which includes eight (8) teams, including _Everton_, _Watford_, _Leicester_, _Bournemouth_, _Crystal Palace_, _Southampton_, _West Ham_, and _Burnley_; those teams are the ones who are negatively correlated with Dimension 2 meaning that they are the ones who scored fewer goals than the other teams.
  + `Cluster3` - This cluster consists of the remaining four(4) teams which include _Chelsea_, _Liverpool_, _Man City_ and _Tottenham_. This cluster is the cluster that has scored the most goals and obtained the least number of cards

  
<br>


```{r, echo=FALSE}
fviz_pca_biplot(PLR_Away_pca,
             col.ind = factor(PLR_Away_clust),  
             #addEllipses = TRUE, 
             #ellipse.type = "confidence",
             legend.title = "Clusters", 
             repel = TRUE)
```
<br>

When teams are playing Away, the clusters are distinguished based on the following:


  + `Cluster1` - This cluster here is the biggest cluster containing eight(8) teams including _Everton_, _Watford_, _Tottenham_, _Man United_, _Arsenal_, _Southampton_, _West Ham_, and _Leicester_. 
  + `Cluster2` - Consisting of four (4) teams, _Chelsea_, _Crystal Palace_, _Bournemouth_ and _Burnley_; those teams are negatively correlated with Dimension 1 meaning that they have received relatively fewer red/yellow cards than the other teams. From this cluster we can also see that Chelsea and Bournmouth have scored more goals than Crystal Palance and Burnley.
  + `Cluster3` - It consists of the remaining two (2) teams, _Liverpool_ and _Man City_. Located on the utmost right of the graph and being highly positively correlated with Dimension 2; those are the teams that have scored the most goals.

  
The plots also show the seed of each cluster denoted by a bigger label in the middle of the cluster.  
 
We can conclude from the graphs above that many of the teams' performance changes depending on whether they are playing Home or Away. For example, Arsenal has scored fewer goals and getting less red cards when playing Away. However, some teams' performance has not change much such as Man City and Liverpool. 

<br>
<br>

## Clustering


For the clustering method, we calculated the Manhattan distance to form clusters of the Home and Away teams. To choose the optimal number of cluster, we have created dendrograms for both Home and Away. For the clustering, we also used Manhattan distance and complete linkage methods. Below are three graphs that we used to select the number of clusters: 

  + `Within sum of squares` 
  + `GAP statistics`
  + `Sihouette graph`

```{r, echo=FALSE, warning=FALSE, include=FALSE}
# matrix of Manhattan distances 
PLR_Home_Mdistance <- dist(PLR_Home_Scale[,], method = "manhattan")

PLR_Away_Mdistance <- dist(PLR_Away_Scale[,], method = "manhattan")
```


```{r, echo=FALSE, warning=FALSE, include=FALSE}
PLR_Home_Mdistance 
```


```{r, echo=FALSE, warning=FALSE, include=FALSE}
PLR_Away_Mdistance 
```


```{r, echo=FALSE}
fviz_nbclust(PLR_Home_Scale[,-1],
             kmeans,
             method = "wss", 
             k.max = 5, verbose = FALSE)
fviz_nbclust(PLR_Home_Scale[,-1],
             kmeans, 
             method = "silhouette", 
             k.max = 5, verbose = FALSE)
fviz_nbclust(PLR_Home_Scale[,-1],
             kmeans,
             method = "gap", 
             k.max = 5, verbose = FALSE)

home.km <- kmeans(PLR_Home_Scale[,-1], centers=1)
```
<br>
  
Looking at the scree plot above and following the so-called "elbow method" we believe that the optimal number of clusters should be three(3) as this decreases the _within variance_ and increases the _between variance_ keeping in mind the complexity of the model.  

Below are the dendrograms that we have created. 


```{r, echo=FALSE, warning=FALSE}
#code to try to change the labels

# PLR_Home_hc <- hclust(PLR_Home_Mdistance, method = "complete")
# plot(PLR_Home_hc, hang=-1, labels_col(value = c("skyblue", "orange", "grey"), k=3), branches_color(value = c("skyblue", "orange", "grey"), k = 3) ) 
# 
# PLR_Away_hc <- hclust(PLR_Away_Mdistance, method = "complete")
# plot(PLR_Away_hc, hang=-1)



###HOME###

dend_home <- PLR_Home_Scale[,] %>% 
  dist("manhattan") %>% 
  hclust(method = "complete") %>% 
  as.dendrogram()

#par(mar=c(1,1,1,7))
dend_home %>%
  set("labels_col", value = c("#5dc263", "#f86565", "#669af0"), k=3) %>%
  set("branches_k_color", value = c("#5dc263", "#f86565", "#669af0"), k = 3) %>%
  plot(horiz=FALSE, axes=FALSE, main ="Home Dendrogram")
abline(v = 350, lty = 2)


###AWAY###

dend_away <- PLR_Away_Scale[,] %>% 
  dist("manhattan") %>% 
  hclust(method = "complete") %>% 
  as.dendrogram() 

#par(mar=c(1,1,1,7))
dend_away %>%
  set("labels_col", value = c("#5dc263", "#f86565", "#669af0"), k=3) %>%
  set("branches_k_color", value = c("#5dc263", "#f86565", "#669af0"), k = 3) %>%
  plot(horiz=FALSE, axes=FALSE, main ="Away Dendrogram")
abline(v = 350, lty = 2)
```


<br>
## Final Comments

Both models provide us with the same clusters. Using PCA, now we know that the the first 2 Dimensions explain more than 72% of the variance for both Home and Away which is provides a good model. From clustering we now know that the optimal number of clusters is three(3) which we have used to add to our biplots in our PCA model to better understand the performance of each team.  
