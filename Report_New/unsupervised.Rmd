<style>
body {
text-align: justify}
</style>

# Unsupervised Learning

```{r, echo = FALSE, message = FALSE}
source(here::here("scripts/setup.R"))
```


In order to have a clearer view on the data, we decided to split the main dataset in to two; one for Home teams and another for Away teams. The goal is to study the performance of each team when they are playing as both Home and Away. 


```{r, echo=FALSE, warning=FALSE}
setwd(here::here("Data_New/"))
premier_league_results <- read_csv("premier_league_results_clean.csv")
```

```{r, echo=FALSE, include=FALSE}
# Divide the dataset into Home and Away

PLR_Home <- premier_league_results %>% select(home_team, full_time_home_goals, half_time_home_goals, home_shots, home_shots_ontarget, home_fouls, home_corners, home_yellow_cards, home_red_cards)

PLR_Away <- premier_league_results %>% select (away_team, full_time_away_goals, half_time_away_goals, away_shots, away_shots_ontarget, away_fouls, away_corners, away_yellow_cards, away_red_cards)

head(PLR_Home) %>% kable(caption = "**Premiere League Results for Home Teams**") %>%
  kable_styling(bootstrap_options = c("striped","hover","condensed"),
                fixed_thead = T) %>% kable_paper() %>%  scroll_box(width = "100%", height = "300px")
```

```{r, echo=FALSE, include=FALSE}
head(PLR_Away) %>% kable(caption = "**Premiere League Results for Away Teams**") %>%
  kable_styling(bootstrap_options = c("striped","hover","condensed"),
                fixed_thead = T) %>% kable_paper() %>%  scroll_box(width = "100%", height = "300px")
```

To have a proper overview of the variables for each team, we have grouped by the teams calculating the median of each variable to obtain a robust grouping. 
After performing the grouping, our dataset was consisting of 14 instances pertaining to the 14 chosen teams and a total of 9 variables which includes the `team name` character column. 
```{r, echo=FALSE, include=FALSE}
# Group by teams

PLR_Home <- PLR_Home %>% group_by(home_team) %>% 
  summarise(Full_time_home_goals = median(full_time_home_goals), 
            Half_time_home_goals = median(half_time_home_goals), 
            Home_shots = median(home_shots),
            Home_shots_ontarget = median(home_shots_ontarget),
            Home_fouls = median(home_fouls),
            Home_corners = median(home_corners),
            Home_yellow_cards = median(home_yellow_cards),
            Home_red_cards = mean(home_red_cards), .groups = 'drop')


PLR_Away <- PLR_Away %>% group_by(away_team) %>% 
  summarise(Full_time_away_goals = median(full_time_away_goals), 
            Half_time_away_goals = median(half_time_away_goals), 
            Away_shots = median(away_shots),
            Away_shots_ontarget = median(away_shots_ontarget),
            Away_fouls = median(away_fouls),
            Away_corners = median(away_corners),
            Away_yellow_cards = median(away_yellow_cards),
            Away_red_cards = mean(away_red_cards), .groups = 'drop')
```



```{r, echo=FALSE, include=FALSE}
head(PLR_Home)%>% kable(caption = "**Median Premiere League Results for Home Teams**") %>%
  kable_styling(bootstrap_options = c("striped","hover","condensed"),
                fixed_thead = T) %>% kable_paper() %>%  scroll_box(width = "100%", height = "300px")
```

```{r, echo=FALSE, include=FALSE}
head(PLR_Away)%>% kable(caption = "**Median Premiere League Results for Away Teams**") %>%
  kable_styling(bootstrap_options = c("striped","hover","condensed"),
                fixed_thead = T) %>% kable_paper() %>%  scroll_box(width = "100%", height = "300px")
```

Our dataset contains 8 numeric variables, each of which has its own scale. As we can see below, the `home_shots` variable has a scale ranging from around 10 to 18, while `full_time_home_goals` has a scale ranging from around 0 to 3. Therefore, we will need to scale the data in order to give the same weight for all variables. 
```{r, echo=FALSE}
# Scale features for home and away

PLR_Home_Scale <- data.frame(scale(PLR_Home[,-1], center=FALSE)) # %>% as_data_frame()
PLR_Away_Scale <- data.frame(scale(PLR_Away[,-1], center=FALSE)) # %>% as_data_frame()


PLR_Home_Scale <- PLR_Home_Scale %>% 
  mutate(home_team = PLR_Home$home_team, .before = Full_time_home_goals )

PLR_Away_Scale <- PLR_Away_Scale %>% 
  mutate(away_team = PLR_Away$away_team, .before = Full_time_away_goals )
```


```{r, echo=FALSE}
rownames(PLR_Home_Scale)[1] <- "Arsenal"
rownames(PLR_Home_Scale)[2] <- "Bournemouth"
rownames(PLR_Home_Scale)[3] <- "Burnley"
rownames(PLR_Home_Scale)[4] <- "Chelsea"
rownames(PLR_Home_Scale)[5] <- "Crystal Palace"
rownames(PLR_Home_Scale)[6] <- "Everton"
rownames(PLR_Home_Scale)[7] <- "Leicester"
rownames(PLR_Home_Scale)[8] <- "Liverpool"
rownames(PLR_Home_Scale)[9] <- "Man City"
rownames(PLR_Home_Scale)[10] <- "Man United"
rownames(PLR_Home_Scale)[11] <- "Southampton"
rownames(PLR_Home_Scale)[12] <- "Tottenham"
rownames(PLR_Home_Scale)[13] <- "Watford"
rownames(PLR_Home_Scale)[14] <- "West Ham"

rownames(PLR_Away_Scale)[1] <- "Arsenal"
rownames(PLR_Away_Scale)[2] <- "Bournemouth"
rownames(PLR_Away_Scale)[3] <- "Burnley"
rownames(PLR_Away_Scale)[4] <- "Chelsea"
rownames(PLR_Away_Scale)[5] <- "Crystal Palace"
rownames(PLR_Away_Scale)[6] <- "Everton"
rownames(PLR_Away_Scale)[7] <- "Leicester"
rownames(PLR_Away_Scale)[8] <- "Liverpool"
rownames(PLR_Away_Scale)[9] <- "Man City"
rownames(PLR_Away_Scale)[10] <- "Man United"
rownames(PLR_Away_Scale)[11] <- "Southampton"
rownames(PLR_Away_Scale)[12] <- "Tottenham"
rownames(PLR_Away_Scale)[13] <- "Watford"
rownames(PLR_Away_Scale)[14] <- "West Ham"
```
The following two tables show the scaled datasets for the first 6 teams. 

```{r, echo=FALSE}
head(PLR_Home_Scale)%>% kable(caption = "**Scaled Premiere League Results for Home Teams**") %>%
  kable_styling(bootstrap_options = c("striped","hover","condensed"),
                fixed_thead = T) %>% kable_paper() %>%  scroll_box(width = "100%", height = "220px")
```
<br>

```{r, echo=FALSE}
head(PLR_Away_Scale)%>% kable(caption = "**Scaled Premiere League Results for Away Teams**") %>%
  kable_styling(bootstrap_options = c("striped","hover","condensed"),
                fixed_thead = T) %>% kable_paper() %>%  scroll_box(width = "100%", height = "220px")
```
<br>
<br>

## Clustering


For the clustering method, we calculated the Manhattan distance to form clusters of the Home and Away teams. To choose the optimal number of cluster, we have created dendrograms for both Home and Away. For the clustering, we also used Manhattan distance and complete linkage methods. Below are three graphs that we used to select the number of clusters: 

  + `Within sum of squares` 
  + `GAP statistics`
  + `Sihouette graph`

```{r, echo=FALSE, warning=FALSE, include=FALSE}
# matrix of Manhattan distances 
PLR_Home_Mdistance <- dist(PLR_Home_Scale[,], method = "manhattan")

PLR_Away_Mdistance <- dist(PLR_Away_Scale[,], method = "manhattan")
```


```{r, echo=FALSE, warning=FALSE, include=FALSE}
PLR_Home_Mdistance 
```


```{r, echo=FALSE, warning=FALSE, include=FALSE}
PLR_Away_Mdistance 
```


```{r, echo=FALSE}
fviz_nbclust(PLR_Home_Scale[,-1],
             kmeans,
             method = "wss", 
             k.max = 5, verbose = FALSE)

home.km <- kmeans(PLR_Home_Scale[,-1], centers=1)
```
<br>
  
Looking at the scree plot above and following the so-called "elbow method" we believe that the optimal number of clusters should be 3 as this decreases the _within variance_ and increases the _between variance_ keeping in mind the complexity of the model.  

Below are the dendrograms that we have created. 


```{r, echo=FALSE, warning=FALSE}

###HOME###

dend_home <- PLR_Home_Scale[,] %>% 
  dist("manhattan") %>% 
  hclust(method = "complete") %>% 
  as.dendrogram()

dend_home %>%
  set("labels_col", value = c("#5dc263", "#f86565", "#669af0"), k=3) %>%
  set("branches_k_color", value = c("#5dc263", "#f86565", "#669af0"), k = 3) %>%
  set("labels_cex", 0.9) %>%  #change the font size
  plot(horiz=FALSE, axes=FALSE, main ="Home Dendrogram")
abline(v = 350, lty = 2)


###AWAY###

dend_away <- PLR_Away_Scale[,] %>% 
  dist("manhattan") %>% 
  hclust(method = "complete") %>% 
  as.dendrogram() 


dend_away %>%
  set("labels_col", value = c("#5dc263", "#f86565", "#669af0"), k=3) %>%
  set("branches_k_color", value = c("#5dc263", "#f86565", "#669af0"), k = 3) %>%
  set("labels_cex", 0.9) %>%
  plot(horiz=FALSE, axes=FALSE, main ="Away Dendrogram")
abline(v = 350, lty = 2)
```

<br>

## Principal Component Analysis (PCA)

For our unsupervised learning model, we decided to work on PCA in order to see the clusters formed and study the important components of each of the Home and Away teams. 

```{r, echo=FALSE, include=FALSE}

PLR_Home_pca <- PCA(PLR_Home_Scale [,-1], ncp = 11, graph = FALSE)
PLR_Away_pca <- PCA(PLR_Away_Scale [,-1], ncp = 11, graph = FALSE)

PLR_Home_pca
```


```{r, echo=FALSE, include=FALSE}
PLR_Away_pca
```

<br>

**_Graphing of Variables_**

The below two graphs show the same 8 variables and for all of the teams when they are both Home and Away. Dimension 1 explains 54.3% of the variance of the data and Dimension 2 explains 21.4% making a total of 75.7% of the variance explained by these two components.  
Both graphs make sense since they show that all variables related to "goals" and "shots" are correlated to Dimension 2 and variables related to "cards" are correlated with Dimension 1. Therefore, we can deduce that Dimension 1 relates to attacking aspects and Dimension 2 relates to defending aspects                      

```{r, echo=FALSE}
grid.arrange(fviz_pca_var(PLR_Home_pca, repel = TRUE, title = "Home Teams", col.var = "Blue"), fviz_pca_var(PLR_Away_pca, repel = TRUE, title = "Away Teams", col.var = "#2EDFC7"), ncol=2)
```
**_Contribution of Variables_**

To  better interpret these dimensions, we can extract the contributions of each features in the dimension. Below, for Dimension1. We can see below the graphs showing contributions of each variable in both Home and Away states. We can conclude that whether the teams are playing Home or Away, there isn't a huge change in the contribution of the variables. 

Further, as shown in the variable graphs, the variables _yellow cards_, _red cards_, and _fouls_ are correlated with Dimension 2. 

```{r, echo=FALSE}
# Extract the contributions of each features in the dimension.
fviz_contrib(PLR_Home_pca, choice = "var", axes = 1, title="Variable Contribution to Dim-1") + fviz_contrib(PLR_Away_pca, choice = "var", axes = 1, title="Variable Contribution to Dim-1", fill = "#2EDFC7")

fviz_contrib(PLR_Home_pca, choice = "var", axes = 2, title="Variable Contribution to Dim-2") + fviz_contrib(PLR_Away_pca, choice = "var", axes = 2, title="Variable Contribution to Dim-2", fill = "#2EDFC7")
```

```{r, echo=FALSE, include=FALSE}
# Did not include the output in the report due to 30 page limit and because the biplot with variables and clusters provides the same output 
fviz_pca_biplot(PLR_Home_pca, repel = TRUE, col.var = "#2E9FDF", col.ind = "#696969", label = "all")
fviz_pca_biplot(PLR_Away_pca, repel = TRUE, col.var = "#2EDFC7", col.ind = "#696969", label = "all")
```

<br>
**_Dimension Analysis_**

The summaries below represent the percentage of contribution of each principle component and the total percentage explained along with various details of each component and individual. The scree plots show a graphical representation of the same percentage of variance for all the Dimensions. As shown in the tables, the first 3 components explain more than 80% of the variance. Considering the 80% rule-of-thumb, we can use the first 3 components. However, if we wanted to follow the Kaiser-Guttman rule, which favors including only dimensions with eigenvalues greater than 1, then we would only consider the first 2 Dimensions.  Therefore, we can reduce the number of dimensions to 2 or 3 and more than 72% of the variance will be explained which is a good percentage. 

<br>
<br>
```{r, echo=FALSE}
PLR_Home_pca$eig %>% kable(caption = "Summary of Variables - Home") %>%
  kable_styling(bootstrap_options = c("striped","hover","condensed"),
                fixed_thead = T) %>% kable_paper() %>%  scroll_box(width = "100%", height = "200px")
```
<br>

```{r, echo=FALSE}
PLR_Away_pca$eig %>% kable(caption = "Summary of Variables - Away") %>%
  kable_styling(bootstrap_options = c("striped","hover","condensed"),
                fixed_thead = T) %>% kable_paper() %>%  scroll_box(width = "100%", height = "200px")
```

<br>
<br>

```{r, echo=FALSE}
ScreeHome <- fviz_eig(PLR_Home_pca, addlabels = TRUE, title ="Home",  ylim=c(0,60), label_size = 5 )# + theme(label.text = element_text(size = 8))

#+ geom_label(label.size = 5)
                                                                              
ScreeAway <- fviz_eig(PLR_Away_pca, addlabels = TRUE, title ="Away", barfill = "#2EDFC7", ylim=c(0,60)) + theme(text = element_text(size = 0.55))
#+ geom_label(label.size = 0.55)

ScreeHome + ScreeAway

```

<br>
<br>
**_Biplots_**

Creating the biplots below, we can combine clustering and pca in order to see each individual, it's relationship with other individuals and variables and create clusters. We have decided to create 3 clusters to decrease the _within variance_ and increase the _between variance_  keeping in mind the complexity of the model, we have created three clusters. 

We can see that, for both Home and Away graphs, the clusters represented are separated by the strength of correlation with each Dimension.

```{r, echo=FALSE}
PLR_Home_hc2 <- hclust(dist(PLR_Home_Scale[,-1], method = "manhattan"), method = "complete")
PLR_Away_hc2 <- hclust(dist(PLR_Away_Scale[,-1], method = "manhattan"), method = "complete")

PLR_Home_clust <- cutree(PLR_Home_hc2, k = 3)
PLR_Away_clust <- cutree(PLR_Away_hc2, k = 3)

fviz_pca_biplot(PLR_Home_pca,
             col.ind = factor(PLR_Home_clust),  
             #addEllipses = TRUE, 
             #ellipse.type = "confidence",
             legend.title = "Clusters", 
             repel = TRUE)
```

<br>
When the team is playing in "Home", the three clusters are distinguished by the following: 

    
  + `Cluster1` - Consists of _Man United_ and _Arsenal_; both of which have relatively a weak positive correlation to Dimension 2 and strong positive correlation to Dimension 1. This means that they relatively have more cards than other teams along with a good number of goals. 
  + `Cluster2` - This is the largest cluster which includes 8 teams, including _Everton_, _Watford_, _Leicester_, _Bournemouth_, _Crystal Palace_, _Southampton_, _West Ham_, and _Burnley_; those teams are the ones who are negatively correlated with Dimension 2 meaning that they are the ones who scored fewer goals than the other teams.
  + `Cluster3` - This cluster consists of the remaining 4 teams which include _Chelsea_, _Liverpool_, _Man City_ and _Tottenham_. This cluster is the cluster that has scored the most goals and obtained the least number of cards

  
<br>


```{r, echo=FALSE}
fviz_pca_biplot(PLR_Away_pca,
             col.ind = factor(PLR_Away_clust),  
             #addEllipses = TRUE, 
             #ellipse.type = "confidence",
             legend.title = "Clusters", 
             repel = TRUE)
```
<br>

When teams are playing Away, the clusters are distinguished based on the following:


  + `Cluster1` - This cluster here is the biggest cluster containing 8 teams including _Everton_, _Watford_, _Tottenham_, _Man United_, _Arsenal_, _Southampton_, _West Ham_, and _Leicester_. 
  + `Cluster2` - Consisting of 4 teams, _Chelsea_, _Crystal Palace_, _Bournemouth_ and _Burnley_; those teams are negatively correlated with Dimension 1 meaning that they have received relatively fewer red/yellow cards than the other teams. From this cluster we can also see that Chelsea and Bournmouth have scored more goals than Crystal Palance and Burnley.
  + `Cluster3` - It consists of the remaining 2 teams, _Liverpool_ and _Man City_. Located on the utmost right of the graph and being highly positively correlated with Dimension 2; those are the teams that have scored the most goals.

  
The plots also show the seed of each cluster denoted by a bigger label in the middle of the cluster.  
 
We can conclude from the graphs above that many of the teams' performance changes depending on whether they are playing Home or Away. For example, Arsenal has scored fewer goals and getting less red cards when playing Away. However, some teams' performance has not change much such as Man City and Liverpool. 


<br>

## Final Comments

Looking at the two models descibed above, both of them provide us with the same clusters. From clustering we now know that the optimal number of clusters is 3 which we have used to add to our biplots in our PCA model to better understand the performance of each team. Using PCA or dimension reduction, now we know that the the first 2 Dimensions explain more than 72% of the variance for both Home and Away which provides a good model.  
